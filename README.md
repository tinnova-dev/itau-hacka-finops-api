# Backend Python - Hackathon API (FastAPI)

Este servi√ßo √© um backend em Python utilizando FastAPI, ideal para desenvolvimento r√°pido de APIs, incluindo aquelas que servem modelos de Intelig√™ncia Artificial. O projeto j√° est√° pronto para uso com LLMs, embeddings, s√©ries temporais, √°udio, vis√£o computacional, tabular, e integra√ß√µes com APIs comerciais e open-source.

## 1. Vis√£o Geral

- **API pronta para produ√ß√£o e hackathons**: FastAPI, Swagger, Makefile, setup.sh, exemplos de uso de IA.
- **Suporte a LLMs, embeddings, √°udio, vis√£o, tabular, s√©ries temporais** (HuggingFace, OpenAI, Groq, etc).
- **Pronto para rodar modelos localmente ou consumir via cloud (Inference API Hugging Face, OpenAI, Groq, etc).**
- **Documenta√ß√£o autom√°tica via Swagger UI e ReDoc.**

## 2. Estrutura de Pastas

```
backend-python/
‚îú‚îÄ‚îÄ Dockerfile           # Imagem Docker para o servi√ßo
‚îú‚îÄ‚îÄ requirements.txt     # Depend√™ncias Python (ML, LLM, cloud, etc)
‚îú‚îÄ‚îÄ Makefile             # Comandos utilit√°rios (run, lint, baixar modelos, etc)
‚îú‚îÄ‚îÄ setup.sh             # Script de setup autom√°tico (instala Python, venv, depend√™ncias)
‚îú‚îÄ‚îÄ download_models.py   # Script para baixar modelos do HuggingFace localmente
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py          # FastAPI principal
‚îÇ   ‚îî‚îÄ‚îÄ model_examples.py # Exemplos de uso de todas as libs/modelos
‚îî‚îÄ‚îÄ README.md            # Este arquivo
```

## 3. Tecnologias e Bibliotecas

- **FastAPI**: API web moderna, r√°pida e com docs autom√°ticas.
- **Uvicorn**: Servidor ASGI para FastAPI.
- **Pydantic**: Valida√ß√£o de dados.
- **transformers, sentence-transformers**: LLMs, embeddings, vis√£o, √°udio.
- **openai, groq**: APIs comerciais e open-source para LLMs.
- **huggingface-hub**: Inference API cloud da Hugging Face.
- **tabpfn, tsai, scikit-learn, pandas, numpy**: Tabular, s√©ries temporais, ML tradicional.
- **langchain, faiss-cpu**: Pipelines de LLMs, busca vetorial.
- **librosa, soundfile, PIL**: √Åudio e vis√£o computacional.

## 4. Como Rodar Localmente

```bash
# Setup autom√°tico (recomendado)
./setup.sh
# Ative o ambiente virtual
source .venv/bin/activate
# Configure as vari√°veis de ambiente (copie o arquivo .env.example para .env e preencha os valores)
cp .env.example .env
# Edite o arquivo .env com suas credenciais
# Rode o servidor
make run
```

### Vari√°veis de Ambiente Necess√°rias

Para o funcionamento completo da API, configure as seguintes vari√°veis de ambiente no arquivo `.env`:

- **AWS Bedrock (Obrigat√≥rio para endpoints FinOpsGPT e Code Review)**:
  - `AWS_BEDROCK_REGION`: Regi√£o da AWS (ex: us-west-2)
  - `AWS_BEDROCK_ACCESS_KEY`: Chave de acesso da AWS (n√£o necess√°rio se estiver usando papel de tarefa ECS)
  - `AWS_BEDROCK_SECRET_KEY`: Chave secreta da AWS (n√£o necess√°rio se estiver usando papel de tarefa ECS)

> **Nota sobre ECS**: Quando executado em um ambiente ECS, a aplica√ß√£o pode usar o papel de tarefa ECS para autentica√ß√£o AWS, n√£o sendo necess√°rio fornecer as chaves de acesso e secreta. Apenas a regi√£o AWS ainda precisa ser configurada.

- **GitHub (Obrigat√≥rio para endpoint Code Review)**:
  - `GITHUB_TOKEN`: Token de acesso ao GitHub

- **Outros Servi√ßos (Opcionais)**:
  - `OPENAI_API_KEY`: Chave da API OpenAI
  - `GROQ_API_KEY`: Chave da API Groq
  - `HF_TOKEN`: Token da Hugging Face

## 5. Como Baixar Modelos do HuggingFace Localmente

```bash
# Exemplo: baixar dois modelos para a pasta ./models
make download MODELS="mistralai/Mistral-7B-Instruct-v0.3 sentence-transformers/all-MiniLM-L6-v2"
```

- Os modelos ficam em `./models/<nome-do-modelo>` e podem ser usados localmente via transformers.

## 6. Documenta√ß√£o Interativa (Swagger)

- Swagger UI: [http://localhost:8000/docs](http://localhost:8000/docs)
- ReDoc: [http://localhost:8000/redoc](http://localhost:8000/redoc)

## 7. Exemplos de Endpoints e Uso de IA

Veja o arquivo `app/model_examples.py` para exemplos pr√°ticos de uso de todas as bibliotecas e modelos suportados.

---

<details>
<summary><strong>üöÄ Como usar modelos da Hugging Face direto da nuvem (Inference API e pipeline cloud)</strong></summary>

### 1. Usando o pipeline do transformers (cloud ou local)

Ideal para PoCs r√°pidas. O pipeline baixa e executa o modelo automaticamente (local ou cloud, se dispon√≠vel):

```python
from transformers import pipeline

pipe = pipeline("text-classification", model="distilbert-base-uncased-finetuned-sst-2-english")
result = pipe("I love this!")
print(result)
```

- Se o modelo n√£o estiver local, ser√° baixado automaticamente.
- Se n√£o houver GPU/infra, pode ser mais lento.

### 2. Usando a Inference API da Hugging Face (cloud)

Ideal para desenvolvimento em locais com internet inst√°vel ou sem GPU local. Voc√™ consome o modelo direto da nuvem da Hugging Face:

```python
from huggingface_hub import InferenceClient

client = InferenceClient(token="<seu_token_hf>")
response = client.text_generation("Qual o prop√≥sito da vida?", model="tiiuae/falcon-7b-instruct")
print(response)
```

- **Vantagens:** N√£o precisa baixar nem rodar o modelo localmente.
- **Desvantagens:** Depende da internet e do plano da Hugging Face (h√° limites gratuitos e pagos).
- **Como obter o token:**
  1. Crie uma conta em <https://huggingface.co>
  2. V√° em "Settings" > "Access Tokens" > "New token" (role: "read")
  3. Use o token no c√≥digo acima.
- **Dica:** Para tasks como classifica√ß√£o, gera√ß√£o de texto, embeddings, etc., basta mudar o m√©todo do `InferenceClient` (ex: `client.text_classification`, `client.feature_extraction`, etc).

</details>

---

## 8. Dicas para Hackathons e Produ√ß√£o

- Se a internet estiver ruim, prefira baixar os modelos antes do evento.
- Se n√£o houver GPU, use modelos menores ou a Inference API.
- O arquivo `model_examples.py` cobre exemplos para todas as libs do requirements.txt (inclusive as comentadas).
- O Makefile e o setup.sh aceleram o setup do ambiente.

## 9. Links √öteis

- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Uvicorn Documentation](https://www.uvicorn.org/)
- [Hugging Face Inference API](https://huggingface.co/inference-api)
- [Transformers Docs](https://huggingface.co/docs/transformers/index)
- [OpenAI API](https://platform.openai.com/docs/)
- [Groq API](https://console.groq.com/docs/openai)
- [LangChain Docs](https://python.langchain.com/docs/get_started/introduction.html)

## 10. Documenta√ß√£o das Bibliotecas e APIs

### APIs e Frameworks

- [FastAPI](https://fastapi.tiangolo.com/)
- [Uvicorn](https://www.uvicorn.org/)
- [Pydantic](https://docs.pydantic.dev/)
- [Docker](https://docs.docker.com/)

### LLMs, NLP, Embeddings

- [Transformers (Hugging Face)](https://huggingface.co/docs/transformers/index)
- [Hugging Face Hub](https://huggingface.co/docs/huggingface_hub/index)
- [sentence-transformers](https://www.sbert.net/)
- [OpenAI Python](https://platform.openai.com/docs/api-reference/introduction)
- [Groq Python](https://github.com/groq/groq-python)
- [LangChain](https://python.langchain.com/docs/get_started/introduction.html)

### Machine Learning, Tabular, S√©ries Temporais

- [scikit-learn](https://scikit-learn.org/stable/)
- [pandas](https://pandas.pydata.org/docs/)
- [numpy](https://numpy.org/doc/)
- [tabpfn](https://github.com/automl/TabPFN)
- [tsai](https://timeseriesai.github.io/tsai/)
- [torch (PyTorch)](https://pytorch.org/docs/stable/index.html)
- [tensorflow](https://www.tensorflow.org/api_docs)
- [faiss-cpu](https://github.com/facebookresearch/faiss)

### √Åudio e Vis√£o Computacional

- [librosa](https://librosa.org/doc/latest/index.html)
- [soundfile](https://pysoundfile.readthedocs.io/en/latest/)
- [Pillow (PIL)](https://pillow.readthedocs.io/en/stable/)

### Desenvolvimento e Qualidade de C√≥digo

- [black](https://black.readthedocs.io/en/stable/)
- [flake8](https://flake8.pycqa.org/en/latest/)
- [mypy](https://mypy.readthedocs.io/en/stable/)
